---
title: "Untitled"
author: "Tata iles"
date: "2023-05-31"
output: html_document
---

# Libraries

```{r include=FALSE}
library(tidyverse)
library(tidymodels)
library(stacks)
library(themis)
library(bonsai)
library(gridExtra)
library(pROC)
```

# Data

```{r}
data <- read.csv("D:/Python-R/Databases/Telco-Customer-Churn.csv")
```

# Preprocessing

```{r}
no_service =colnames(data)[7:14]
data <- data %>% select(-1) %>%
  mutate(SeniorCitizen=as.factor(SeniorCitizen)) %>%
  mutate_at(no_service,
            function(x) ifelse(
              x %in% c("No internet service","No phone service","No"),
              "No","Yes")) %>%
  mutate_if(is.character, as.factor) 

data$Churn <- relevel(data$Churn, ref = "Yes")
```

# Unbalanced data

```{r}
perc <- data %>%
  count(Churn) %>%
  mutate(n=paste(round(n*100/nrow(data),2),"%")) %>% pull(n)
data %>% count(Churn) %>% ggplot(aes(n,Churn,fill=Churn))+geom_col(width = 0.5) +
  geom_text(aes(label = perc)) + scale_fill_grey() + theme_light()
```

# Data split

```{r}
set.seed(123)

data_split <- data %>% initial_split()

train <- data_split %>% training()
test <- data_split %>% testing()

folds <- vfold_cv(train, v = 5)
```

# Metric set

```{r}
metric <- metric_set(sens, precision,yardstick::spec, j_index)
```

# Recipe

```{r}
# simple recipe
unbalanced_rec <- train %>% recipe(Churn~.) %>% 
  step_dummy(all_nominal_predictors()) %>%
  step_mutate(log_avg=log(TotalCharges/MonthlyCharges))%>%
  step_normalize(c("tenure","TotalCharges","MonthlyCharges")) 

# adasyn recipe to rebalance the data
adasyn_rec <- unbalanced_rec %>% 
  step_adasyn(Churn,over_ratio = 0.8)
```

# Rebalanced data

```{r}
data_processed <- prep(adasyn_rec,data=train) %>%
  juice()


before_oversampling <- ggplot(train, aes(x = Churn,fill = Churn)) +
  geom_bar() +
  labs(title = "Distribution before Adasyn",
       x = "Churn",
       y = "Count")

after_oversampling <- ggplot(data_processed, aes(x = Churn,fill=Churn)) +
  geom_bar() +
  labs(title = "Distribution after Adasyn",
       x = "Churn",
       y = "Count") 

grid.arrange(before_oversampling, after_oversampling, nrow = 1)
```

# Models set

```{r}
# simple logistic regression no hyperparameters
log_spec <- logistic_reg() %>%
  set_engine("glm") %>% 
  set_mode("classification")

#random forest
rf_spec <- rand_forest(trees = tune(), mtry = tune(), min_n = tune()) %>%
  set_mode("classification") %>%
  set_engine("ranger")

#xgboost
xgb_spec <- boost_tree(mtry = tune(), trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune()) %>%
  set_mode("classification") %>%
  set_engine("xgboost")

#SVM with radial base model 
svm_spec <- svm_rbf() %>%
set_mode("classification") %>%
set_engine("kernlab")

#a simple neural network for variaty 
mlp_spec <-
  mlp(hidden_units = tune(), epochs = tune()) %>%
  set_mode("classification") %>%
  set_engine("nnet")
```

# Setting up workflows

```{r}
#global workflow 
global_wflow <- 
  workflow() %>% 
  add_recipe(unbalanced_rec)

log_wflow <- global_wflow %>% add_model(log_spec)

rf_wflow <- global_wflow %>% add_model(rf_spec)

xgb_wflow <- global_wflow %>% add_model(xgb_spec)

svm_wflow <- global_wflow %>% add_model(svm_spec)

mlp_wflow <- global_wflow %>% add_model(mlp_spec)
```

# Grids

```{r}
ctrl_grid <- control_stack_grid()
ctrl_res <- control_stack_resamples()
```

```{r}
#grid for random forest 
grid <- grid_latin_hypercube(mtry(c(1, 5)), 
                         trees(c(100,1000)), 
                         min_n(), 
                         size = 10)

#grid for the boosting algorithm
grid_boost <- grid_latin_hypercube(mtry(c(1, 5)), 
                         trees(c(100,1000)), 
                         min_n(),
                         tree_depth(),
                         learn_rate(),
                         size = 10)

#random grid for the neural network
grid_nnet <- grid_random(hidden_units(),epochs())
```

# Stack 1

To check models performance without rebalancing the data

## Hyperparameter tuning

```{r}
rf_res_1 <- 
  tune_grid(
    object = rf_wflow, 
    resamples = folds, 
    grid = grid,
    control = ctrl_grid
  )


xgb_res_1 <- 
  tune_grid(
    object = xgb_wflow, 
    resamples = folds, 
    grid = grid_boost,
    control = ctrl_grid
  )

#no parameter to tune so it will fit the resamples from the cross validation
svm_res_1 <- 
  tune_grid(
    object = svm_wflow, 
    resamples = folds, 
    grid = 1,
    control = ctrl_grid
  )


mlp_res_1 <- 
  tune_grid(
    object = mlp_wflow, 
    resamples = folds, 
    grid = grid_nnet,
    control = ctrl_grid
  )
```

## Stacking

```{r}
#creating a stack without logestic regression
stack <- 
  stacks(mode = "classification") %>%
  add_candidates(rf_res_1) %>%
  add_candidates(xgb_res_1) %>%
  add_candidates(svm_res_1) %>%
  add_candidates(mlp_res_1) %>%
  blend_predictions() %>%
  fit_members()

autoplot(stack)
```

```{r}
stack
```

## Visualisations

```{r}
autoplot(stack, type = "weights")
```

Roc-auc of every stack member

```{r}
pred <-
  test %>%
  bind_cols(predict(stack, ., type = "prob"))

pred <-
  test %>%
  select(Churn) %>%
  bind_cols(
    predict(
      stack,
      test,
      type = "class",
      members = TRUE
      )
    )

map(
  colnames(pred),
  ~mean(pred$Churn == pull(pred, .x))
) %>%
  set_names(colnames(pred)) %>%
  as_tibble() %>%
  pivot_longer(c(everything(), -Churn)) %>% rename(roc_auc = value) -> auc_stacks_1
auc_stacks_1
```

```{r}
auc_stacks_1 %>% ggplot(aes(name,roc_auc)) + geom_point() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
```

. Logestic regression imbalanced data ---\> best roc_auc

. Low sensitvity 0.55

. Very low j-index (data not balanced

```{r}
log_wflow1 <- global_wflow %>% add_model(log_spec)

rbind(log_wflow1 %>% last_fit(data_split) %>% collect_metrics() %>% select(-.config),
      log_wflow1 %>% last_fit(data_split) %>% collect_predictions() %>% metric(truth = Churn,estimate = .pred_class))
```

# Stack 2

Compare models performances after rebalancing

```{r}
# updating the workflow's recipes to rebalance data

log_wflow <- log_wflow1 %>% update_recipe(adasyn_rec)

rf_wflow <- rf_wflow %>% update_recipe(adasyn_rec)

svm_wflow <- svm_wflow %>% update_recipe(adasyn_rec)

xgb_wflow <- xgb_wflow %>% update_recipe(adasyn_rec)

mlp_wflow <- mlp_wflow %>% update_recipe(adasyn_rec)
```

## Hyperparameter tuning

```{r}
log_res_2 <- 
  tune_grid(
    object = log_wflow, 
    resamples = folds, 
    grid = 0,
    control = ctrl_grid
  )

rf_res_2 <- 
  tune_grid(
    object = rf_wflow, 
    resamples = folds, 
    grid = grid,
    control = ctrl_grid
  )


xgb_res_2 <- 
  tune_grid(
    object = xgb_wflow, 
    resamples = folds, 
    grid = grid_boost,
    control = ctrl_grid
  )


svm_res_2 <- 
  tune_grid(
    object = svm_wflow, 
    resamples = folds, 
    grid = 1,
    control = ctrl_grid
  )


mlp_res_2 <- 
  tune_grid(
    object = mlp_wflow, 
    resamples = folds, 
    grid = grid_nnet,
    control = ctrl_grid
  )
```

## Stacking

```{r}
stack_2 <- 
  stacks(mode = "classification") %>%
  add_candidates(log_res_2) %>%
  add_candidates(rf_res_2) %>%
  add_candidates(xgb_res_2) %>%
  add_candidates(svm_res_2) %>%
  add_candidates(mlp_res_2) %>%
  blend_predictions() %>%
  fit_members()
  
autoplot(stack_2)
```

## Visualisations

```{r}
autoplot(stack_2, type = "weights")
```

```{r}
stack_2
```

Stack performance (roc-auc of every stack member)

```{r}
pred <-
  test %>%
  bind_cols(predict(stack_2, ., type = "prob"))

pred <-
  test %>%
  select(Churn) %>%
  bind_cols(
    predict(
      stack_2,
      test,
      type = "class",
      members = TRUE
      )
    )

map(
  colnames(pred),
  ~mean(pred$Churn == pull(pred, .x))
) %>%
  set_names(colnames(pred)) %>%
  as_tibble() %>%
  pivot_longer(c(everything(), -Churn)) %>% rename(roc_auc = value) -> auc_stacks_2

auc_stacks_2
```

```{r}
auc_stacks_2 %>% ggplot(aes(name,roc_auc)) + geom_point() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

# Plus

## Light-gbm

. Light gbm gives better stability over the metrics

. better sensitvity

. higher j-index

. slightly lower roc-auc

```{r}
light_gbm_spec <- boost_tree(
  engine = "lightgbm",
  mode = "classification",
  trees = 150,           
  tree_depth = 7,        
  learn_rate = 0.0042,
  mtry=3)

light_gbm_wflow <- global_wflow %>% 
  add_model(light_gbm_spec) %>%
  update_recipe(adasyn_rec)

rbind(light_gbm_wflow %>% last_fit(data_split) %>% collect_metrics() %>% select(-.config),
      light_gbm_wflow %>% last_fit(data_split) %>% collect_predictions() %>% metric(truth = Churn,estimate = .pred_class))
```

## XGboost

. XGboost with these parameters gives higher j-index

. Higher sens and higher spec

. But lower precision

```{r}
xgb_spec_no_tune <- boost_tree(mtry = 3, trees = 150, min_n = 50, tree_depth = 7, learn_rate = 0.0042) %>%
  set_mode("classification") %>%
  set_engine("xgboost")

xgb_wflow_no_tune <- global_wflow %>% 
  add_model(xgb_spec_no_tune) %>%
  update_recipe(adasyn_rec)

rbind(xgb_wflow_no_tune %>% last_fit(data_split) %>% collect_metrics() %>% select(-.config),
      xgb_wflow_no_tune %>% last_fit(data_split) %>% collect_predictions() %>% metric(truth = Churn,estimate = .pred_class))
```

# ROC-curves

```{r}
best <- select_best(mlp_res_2)
mlp_f <- mlp(hidden_units = best$hidden_units,epochs=best$epochs) %>%
  set_mode("classification") %>% set_engine("nnet")
final_workflow <- global_wflow %>%
  add_model(mlp_f)
```

```{r}
light_gbm_wflow %>%
  last_fit(data_split) %>% 
  collect_predictions() %>% 
  roc(Churn,.pred_Yes) -> g

xgb_wflow_no_tune %>%
  last_fit(data_split) %>% 
  collect_predictions() %>% roc(Churn,.pred_Yes) -> g1

log_wflow1 %>% last_fit(data_split) %>%
  collect_predictions() %>%
  roc(Churn,.pred_Yes) -> g2

final_workflow %>%
  last_fit(data_split) %>% 
  collect_predictions() %>% 
  roc(Churn,.pred_Yes) -> g3

svm_wflow %>%
  last_fit(data_split) %>% 
  collect_predictions() %>% roc(Churn,.pred_Yes) -> g4

gbm_data <- data.frame(sens = g$sensitivities,spec = g$specificities)
xgb_data <- data.frame(sens = g1$sensitivities,spec = g1$specificities)
log_data <- data.frame(sens = g2$sensitivities,spec = g2$specificities)
mlp_data <- data.frame(sens = g3$sensitivities,spec = g3$specificities)
svm_data <- data.frame(sens = g4$sensitivities,spec = g4$specificities)

  ggplot() +
    geom_line(data = gbm_data,aes(1-spec,sens),col="blue") +
    geom_line(aes(x=c(0,1),y=c(0,1)),col="#f52359") +
    geom_line(data = xgb_data,aes(1-spec,sens),col="green") +
    geom_line(data = log_data,aes(1-spec,sens),col="white") + 
    geom_line(data = mlp_data,aes(1-spec,sens),col="coral") +
    geom_line(data = svm_data,aes(1-spec,sens),col="#f45521") +
    theme_dark() +
    labs(x = "1 - Specificity", y = "Sensitivity", col = "Model") +
    scale_color_manual(values = c("GBM" = "blue", "XGB" = "green", "Logistic   Regression" = "white", "MLP" = "coral")) +
  guides(col = guide_legend(title = "Model")) 
```
